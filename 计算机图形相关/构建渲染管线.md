渲染管线

概述

1、Application应用层

准备好场景数据：照相机的位置、视椎体、场景中的模型、使用的光源等；



为了提高渲染性能，需要进行粗粒度剔除（cullling），把不可见的物体剔除；



设置每个模型的渲染状态，如：使用的材质（漫反射颜色、高光反射颜色）、使用的纹理、使用的Shader等；



主要任务是将需要在屏幕上显示的渲染图元（rendering primitives）、摄像机位置、光照信息等输入到几何阶段。渲染图元可以是点、线、三角形等；



2.Geometry Processing几何处理阶段

GPU上的几何处理阶段负责大部分的逐三角形和逐顶点的操作，并且细分为以下步骤依次执行



r.set_model(get_model_matrix(angle));

r.set_view(get_view_matrix(eye_pos));

r.set_projection(get_projection_matrix(45, 1, 0.1, 50));



2.1 Vertex Shading 顶点着色



顶点着色分为两个任务，计算顶点位置和储存编程人员可能用于顶点输出数据的东西（法线和纹理坐标）。随着现代GPU到来，顶点着色器（可编程顶点处理单元）是更加通用的专门设置每个顶点数据的单元。



2.1.1 模型顶点位置的计算



从显示到屏幕上的过程中，一个模型需要在多个不同的空间或坐标系统中进行转换，最初模型处于他的模型空间，通过 换位置和方向，实际上变换的是模型的顶点和法线，经过变换后的物体处于世界空间中（并且拥有自己的世界坐标）。而相机在世界空间拥有自己的位置与朝向，所以使用视图变换view transform来变换相机与连带着的其他模型。（目的是将相机放置在坐标原点，并对准z轴的负方向）。实际上，模型、视图变换都可以用一个4X4的矩阵来表示



//移动到原点

​    Eigen::Matrix4f move;

​    move << 1, 0, 0, -(r + l) / 2,

​        0, 1, 0, -(t + b) / 2,

​        0, 0, 1, -(n + f) / 2,

​        0, 0, 0, 1;



2.1.2 有关顶点着色的数据



生成一个写实的场景，不仅仅需要渲染对象的形状和位置，还需要对外观进行建模。也就是shading，这包括对象上多个点的着色方程。这些计算一部分在顶点几何处理阶段，一部分在逐像素阶段。顶点需要储存好材质数据，例如点坐标、法线、颜色、向量、纹理坐标等其他计算着色方程所需要的数值信息。顶点着色结果（上述储存的）会被传递到光栅化与像素处理阶段，去做进一步的插值与计算表面的颜色。



2.2 Projection 投影



投影由GPU上的顶点着色器完成，共有两种常用的投影方法，分别是正交投影orthgraphic projection和透视投影perspective projection。投影都是以矩阵的形式表达，并且首先会将视体变换为单位立方体。在经历过无论哪种投影的变换后，物体都处在了齐次坐标，也就是裁剪坐标culling coordinate中。需要注意的是，在投影之后，将三维空间的点投影到了二维平面上，其顶点的z坐标会存在z缓冲z-buffer中



//缩放到-1,1这样的立方体中 --总大小为2*2*2

​    Eigen::Matrix4f scale;

​    scale << 2 / (r - l), 0, 0, 0,

​        0, 2 / (t - b),0,0,

​        0, 0, 2 / (n - f), 0,

​        0, 0, 0, 1;



2.3 Culling 裁剪



只有在视体范围（可视的范围内）内的图元需要进入光栅化的阶段。所以需要进行裁剪的就是部分处于视体内的图元。裁剪的方式例如物体一个顶点在视体范围外，就用视体和线段的交点来替换处于视体外的顶点。



2.4 Screen mapping 屏幕映射



进入到这个阶段时候的坐标仍然是三维的，屏幕映射对每个图元的x和y坐标变换来构建屏幕坐标。屏幕坐标（x、y）和z坐标被称为windows坐标windows coordinate。因为投影变换后图元都在单位立方体中，屏幕映射需要在显示的屏幕上找到相应的坐标，屏幕映射也可以理解为平移与缩放的结合。在屏幕映射结束后，新的x和y坐标就成为了屏幕坐标，z坐标会被映射到[z1,z2](在openGL中，对应[-1,1]，在DirectX中对应[0,1]）。windows坐标和重映射（一个范围的数值映射到另一个范围）的z值送入光栅化的阶段



3、Rasterization  光栅化

对三角形中的每个像素进行采样



光栅化，又称为扫描转换（scan transform），是将屏幕空间的二维顶点（每个顶点的Z值、着色信息、纹理坐标、顶点颜色、法线方向、视角方向等）转换到屏幕上的像素；光栅化也被认为是几何处理和像素处理的连接点；光栅化阶段有两个最重要的目标：计算每个图元覆盖了哪些像素，以及为这些像素计算它们的颜色；



Triangle Processing 三角形处理



将平面上的点连接为一个个三角形，由硬件的固定管线完成；计算三角形的微分、边缘方程和误差，用于三角形遍历，以及对几何阶段产生的着色数据进行插值；在这一阶段，GPU会将转换完的顶点连接成线，组装成一个个三角形，这些三角形被称为图元，因此图元装配（premitive Assembly）这一步骤又被称为三角形设置。检查每一个像素是否被一个三角网格所覆盖。如果被覆盖的话，就会生成一个片元（fragment）。而这样一个找到哪些像素被三角网格覆盖的过程就是三角形遍历；三角形遍历阶段会根据上一个阶段计算结果来判断一个三角网格覆盖了哪些像素，并使用三角网格3个顶点的顶点信息对整个覆盖区域的像素进行插值。这一步输出得到一个片元序列，这里一个片元并不是真正意义上的像素，而是包含了很多状态的集合，这些状态用于计算每个像素的最终颜色。这些状态包括但不限于：屏幕坐标、深度信息、从几何阶段输出的顶点信息，例如法线、纹理坐标等；



4.Fragment Processing 像素处理

缓存每个点的深度



这个阶段之前的处理，确定了三角形内部或者其他图元的像素，像素处理主要分为像素着色（pixel shading）与合并（merging）两个阶段; 像素处理阶段是基于逐像素或者逐采样点处理的；



Fragment Shading 片元着色



作用于顶点与像素



完全可编程的，用于实现逐片元的着色操作；前面的阶段并不会影响屏幕上每个像素的颜色值，而是会产生一系列的数据信息，用于表述一个三角网格是怎么覆盖每个像素的；每个片元就负责存储这样一系列数据。像素着色的输入是插值着色数据，而它的输出是一个或者多个颜色值；这一阶段由可编程GPU内核决定，开发者需要写一套像素着色程序（OpenGL中为fragment shader）进行逐像素计算；这阶段可以完成很多渲染技术，比如，纹理采样；像素着色器的局限是：仅可以影响单个片元。也就是说，当执行像素着色器时，它不可以将自己的任何结果直接发送给它的邻居；其中一个情况例外，片元着色器可以访问到导数信息；



Texture mapping 纹理映射



完成纹理映射 （uv坐标）



颜色缓冲中存储着每个像素的信息，合并阶段是将像素着色器产生的片元颜色以当前存储在缓冲中的颜色合并；这个阶段也称为ROP，即raster operation(pipeline)或者渲染输出单元。和着色阶段不同，执行此阶段的GPU子单元通常不完全可编程；但是，它可以高度可配置；负责执行操作，如：修改颜色、深度缓冲、进行混合等；这一阶段的主要任务：1）决定片元的可见性；涉及的测试工作：深度测试、模板测试等；未通过测试，则舍弃该片元；不管有没有通过模板测试，都可以根据模板测试修改模板缓冲区中的值；模板测试和深度测试的函数都是由开发者设计的；



【补充】渲染管线深度测试是zbuffer吗? 不是；深度测试的结果深度值更新缓存在zbuffer；由于Z-Buffer仅仅储存屏幕上每一个点的单独深度，无法应用在部分透明的图元上，半透明图元必须在渲染了所有不透明图元之后渲染，且按照从后到前的顺序，或者使用其他不依赖顺序算法。全透明渲染是z缓冲区的主要缺点之一。



2）如果一个片元通过了所有的测试，就需要把这个片元的颜色值和已经存储在颜色缓冲区中的颜色进行合并，或者说是混合；每个像素的颜色信息被存储在颜色缓冲区，当执行这次渲染，颜色缓冲中已经存在上次渲染结果，应该怎么进行合并呢？对于不透明物体，开发者可以关闭Blend操作，这样片元着色器计算得到的颜色值可以直接覆盖掉颜色缓冲区的像素值；但是对于半透明物体，需要使用混合操作让物体看起来透明；为了充分提高性能，GPU希望尽可能早的知道哪些片元被舍弃的，然后就不需要使用fs计算它们的颜色；在Unity给出的渲染流水线上，它的深度测试是在片元着色器之前，这种将深度测试提前执行的技术通常称为Early-Z技术；屏幕显示的就是颜色缓冲区中的颜色值，但是为了避免我们看到那些进行光栅化的图元，GPU会使用双重缓冲的策略；这就意味着，对场景的渲染是在幕后进行的，即后置缓冲；一旦场景已经被渲染到后置缓冲中，GPU就会交换后置缓冲区和前置缓冲区中的内容，而前置缓冲区是之前显示在屏幕上的图像，由此，保证了我们看到的画面是连续的；



Display 完成